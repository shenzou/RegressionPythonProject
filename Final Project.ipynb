{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip3 install pandas\n",
    "#!sudo pip3 install sklearn\n",
    "#!sudo pip3 install graphviz\n",
    "#!sudo pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importation du dataset contenant les données débloatées \n",
    "(Plusieurs colonnes normalisées (caller_id, opened_by, sys_created_by, sys_updated_by, location, category, subcategory, u_symptom, cmdb_ci, assignement_group, assigned_to, problem_id, rfc, vendor, closed_code, resolved_by ne contiennent plus que des valeurs numériques, sans perte d'information). Les \"?\" ont été remplacés par des cases vides. \n",
    "De plus, les variables de type object ou string ont été converties en float ou int, suivant le cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>incident_state</th>\n",
       "      <th>active</th>\n",
       "      <th>reassignment_count</th>\n",
       "      <th>reopen_count</th>\n",
       "      <th>sys_mod_count</th>\n",
       "      <th>made_sla</th>\n",
       "      <th>caller_id</th>\n",
       "      <th>opened_by</th>\n",
       "      <th>opened_at</th>\n",
       "      <th>...</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>u_priority_confirmation</th>\n",
       "      <th>notify</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>rfc</th>\n",
       "      <th>vendor</th>\n",
       "      <th>closed_code</th>\n",
       "      <th>resolved_by</th>\n",
       "      <th>resolved_at</th>\n",
       "      <th>closed_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>New</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29/2/2016 01:16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>5/3/2016 12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29/2/2016 01:16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>5/3/2016 12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29/2/2016 01:16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>5/3/2016 12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INC0000045</td>\n",
       "      <td>Closed</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>29/2/2016 01:16</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>29/2/2016 11:29</td>\n",
       "      <td>5/3/2016 12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INC0000047</td>\n",
       "      <td>New</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2403.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>29/2/2016 04:40</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1/3/2016 09:52</td>\n",
       "      <td>6/3/2016 10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141707</th>\n",
       "      <td>INC0120835</td>\n",
       "      <td>Closed</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16/2/2017 09:09</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16/2/2017 09:53</td>\n",
       "      <td>16/2/2017 09:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141708</th>\n",
       "      <td>INC0121064</td>\n",
       "      <td>Active</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16/2/2017 14:17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141709</th>\n",
       "      <td>INC0121064</td>\n",
       "      <td>Active</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16/2/2017 14:17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141710</th>\n",
       "      <td>INC0121064</td>\n",
       "      <td>Resolved</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16/2/2017 14:17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141711</th>\n",
       "      <td>INC0121064</td>\n",
       "      <td>Closed</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>116.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16/2/2017 14:17</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Do Not Notify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "      <td>16/2/2017 16:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141712 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            number incident_state  active  reassignment_count  reopen_count  \\\n",
       "0       INC0000045            New    True                   0             0   \n",
       "1       INC0000045       Resolved    True                   0             0   \n",
       "2       INC0000045       Resolved    True                   0             0   \n",
       "3       INC0000045         Closed   False                   0             0   \n",
       "4       INC0000047            New    True                   0             0   \n",
       "...            ...            ...     ...                 ...           ...   \n",
       "141707  INC0120835         Closed   False                   1             0   \n",
       "141708  INC0121064         Active    True                   0             0   \n",
       "141709  INC0121064         Active    True                   1             0   \n",
       "141710  INC0121064       Resolved    True                   1             0   \n",
       "141711  INC0121064         Closed   False                   1             0   \n",
       "\n",
       "        sys_mod_count  made_sla  caller_id  opened_by        opened_at  ...  \\\n",
       "0                   0      True     2403.0        8.0  29/2/2016 01:16  ...   \n",
       "1                   2      True     2403.0        8.0  29/2/2016 01:16  ...   \n",
       "2                   3      True     2403.0        8.0  29/2/2016 01:16  ...   \n",
       "3                   4      True     2403.0        8.0  29/2/2016 01:16  ...   \n",
       "4                   0      True     2403.0      397.0  29/2/2016 04:40  ...   \n",
       "...               ...       ...        ...        ...              ...  ...   \n",
       "141707              4      True      116.0       12.0  16/2/2017 09:09  ...   \n",
       "141708              0      True      116.0       12.0  16/2/2017 14:17  ...   \n",
       "141709              1      True      116.0       12.0  16/2/2017 14:17  ...   \n",
       "141710              2      True      116.0       12.0  16/2/2017 14:17  ...   \n",
       "141711              3      True      116.0       12.0  16/2/2017 14:17  ...   \n",
       "\n",
       "        knowledge u_priority_confirmation         notify problem_id  rfc  \\\n",
       "0            True                   False  Do Not Notify        NaN  NaN   \n",
       "1            True                   False  Do Not Notify        NaN  NaN   \n",
       "2            True                   False  Do Not Notify        NaN  NaN   \n",
       "3            True                   False  Do Not Notify        NaN  NaN   \n",
       "4            True                   False  Do Not Notify        NaN  NaN   \n",
       "...           ...                     ...            ...        ...  ...   \n",
       "141707      False                    True  Do Not Notify        NaN  NaN   \n",
       "141708      False                   False  Do Not Notify        NaN  NaN   \n",
       "141709      False                   False  Do Not Notify        NaN  NaN   \n",
       "141710      False                    True  Do Not Notify        NaN  NaN   \n",
       "141711      False                    True  Do Not Notify        NaN  NaN   \n",
       "\n",
       "        vendor  closed_code  resolved_by      resolved_at        closed_at  \n",
       "0          NaN          5.0        149.0  29/2/2016 11:29   5/3/2016 12:00  \n",
       "1          NaN          5.0        149.0  29/2/2016 11:29   5/3/2016 12:00  \n",
       "2          NaN          5.0        149.0  29/2/2016 11:29   5/3/2016 12:00  \n",
       "3          NaN          5.0        149.0  29/2/2016 11:29   5/3/2016 12:00  \n",
       "4          NaN          5.0         81.0   1/3/2016 09:52   6/3/2016 10:00  \n",
       "...        ...          ...          ...              ...              ...  \n",
       "141707     NaN          9.0          9.0  16/2/2017 09:53  16/2/2017 09:53  \n",
       "141708     NaN          6.0          9.0  16/2/2017 16:38  16/2/2017 16:38  \n",
       "141709     NaN          6.0          9.0  16/2/2017 16:38  16/2/2017 16:38  \n",
       "141710     NaN          6.0          9.0  16/2/2017 16:38  16/2/2017 16:38  \n",
       "141711     NaN          6.0          9.0  16/2/2017 16:38  16/2/2017 16:38  \n",
       "\n",
       "[141712 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La colonne \"caused_by\" a été supprimée car vide\n",
    "dataset = pd.read_csv(\"incident_event_log_debloated.csv\", sep=',', encoding='UTF-8')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number                      object\n",
       "incident_state              object\n",
       "active                        bool\n",
       "reassignment_count           int64\n",
       "reopen_count                 int64\n",
       "sys_mod_count                int64\n",
       "made_sla                      bool\n",
       "caller_id                  float64\n",
       "opened_by                  float64\n",
       "opened_at                   object\n",
       "sys_created_by             float64\n",
       "sys_created_at              object\n",
       "sys_updated_by               int64\n",
       "sys_updated_at              object\n",
       "contact_type                object\n",
       "location                   float64\n",
       "category                   float64\n",
       "subcategory                float64\n",
       "u_symptom                  float64\n",
       "cmdb_ci                    float64\n",
       "impact                      object\n",
       "urgency                     object\n",
       "priority                    object\n",
       "assignment_group           float64\n",
       "assigned_to                float64\n",
       "knowledge                     bool\n",
       "u_priority_confirmation       bool\n",
       "notify                      object\n",
       "problem_id                 float64\n",
       "rfc                         object\n",
       "vendor                      object\n",
       "closed_code                float64\n",
       "resolved_by                float64\n",
       "resolved_at                 object\n",
       "closed_at                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversion des types de données erronés\n",
    "On va supprimer certaines colonnes qui sont inutiles pour notre régression, comme l'ID de l'incident et le RFC.\n",
    "Pandas a également mal interprété le type de données de certaines colonnes, on applique donc une correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active                38716\n",
      "New                   36407\n",
      "Resolved              25751\n",
      "Closed                24985\n",
      "Awaiting User Info    14642\n",
      "Awaiting Vendor         707\n",
      "Awaiting Problem        461\n",
      "Awaiting Evidence        38\n",
      "-100                      5\n",
      "Name: incident_state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop(columns=\"rfc\")\n",
    "dataset.active = dataset.active.astype(int)\n",
    "dataset.made_sla = dataset.made_sla.astype(int)\n",
    "print(dataset.incident_state.value_counts())\n",
    "incident_state_dico = {\"New\":0, \"Resolved\":1, \"Closed\":2, \"Active\":3, \"Awaiting User Info\":4, \"Awaiting Vendor\":5, \"Awaiting Problem\":6, \"Awaiting Evidence\":7, \"-100\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset.replace({\"incident_state\": incident_state_dico})\n",
    "dataset.incident_state = dataset.incident_state.map(incident_state_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone             140462\n",
      "Self service         995\n",
      "Email                220\n",
      "IVR                   18\n",
      "Direct opening        17\n",
      "Name: contact_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "141707    2\n",
       "141708    2\n",
       "141709    2\n",
       "141710    2\n",
       "141711    2\n",
       "Name: contact_type, Length: 141712, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.contact_type.value_counts())\n",
    "contact_type_dico = {\"Phone\":0, \"Self service\":1, \"Email\":2, \"IVR\":3, \"Direct opening\":4}\n",
    "dataset.contact_type = dataset.contact_type.map(contact_type_dico)\n",
    "dataset.contact_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - Medium    134335\n",
      "3 - Low         3886\n",
      "1 - High        3491\n",
      "Name: impact, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.impact.value_counts())\n",
    "impact_dico = {\"2 - Medium\":2, \"3 - Low\":3, \"1 - High\":1}\n",
    "dataset.impact = dataset.impact.map(impact_dico)\n",
    "dataset.urgency = dataset.urgency.map(impact_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 - Moderate    132452\n",
      "4 - Low           4030\n",
      "2 - High          2972\n",
      "1 - Critical      2258\n",
      "Name: priority, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.priority.value_counts())\n",
    "priority_dico = {\"3 - Moderate\":3, \"4 - Low\":4, \"2 - High\":2, \"1 - Critical\":1}\n",
    "dataset.priority = dataset.priority.map(priority_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.knowledge = dataset.knowledge.astype(int)\n",
    "dataset.u_priority_confirmation = dataset.u_priority_confirmation.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do Not Notify    141593\n",
      "Send Email          119\n",
      "Name: notify, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.notify.value_counts())\n",
    "notify_dico = {\"Do Not Notify\":0, \"Send Email\":1}\n",
    "dataset.notify = dataset.notify.map(notify_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8s          167\n",
      "Vendor 1     69\n",
      "Vendor 3      6\n",
      "Vendor 2      2\n",
      "Name: vendor, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset.vendor.value_counts())\n",
    "vendor_dico = {\"8s\":0, \"Vendor 1\":1, \"Vendor 2\":2, \"Vendor 3\":3}\n",
    "dataset.vendor = dataset.vendor.map(vendor_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number                      object\n",
       "incident_state               int64\n",
       "active                       int64\n",
       "reassignment_count           int64\n",
       "reopen_count                 int64\n",
       "sys_mod_count                int64\n",
       "made_sla                     int64\n",
       "caller_id                  float64\n",
       "opened_by                  float64\n",
       "opened_at                   object\n",
       "sys_created_by             float64\n",
       "sys_created_at              object\n",
       "sys_updated_by               int64\n",
       "sys_updated_at              object\n",
       "contact_type                 int64\n",
       "location                   float64\n",
       "category                   float64\n",
       "subcategory                float64\n",
       "u_symptom                  float64\n",
       "cmdb_ci                    float64\n",
       "impact                       int64\n",
       "urgency                      int64\n",
       "priority                     int64\n",
       "assignment_group           float64\n",
       "assigned_to                float64\n",
       "knowledge                    int64\n",
       "u_priority_confirmation      int64\n",
       "notify                       int64\n",
       "problem_id                 float64\n",
       "vendor                     float64\n",
       "closed_code                float64\n",
       "resolved_by                float64\n",
       "resolved_at                 object\n",
       "closed_at                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141712, 34)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. On cherche le temps de résolution pour chaque ligne\n",
    "Pour chaque ligne, on calcule la différence de temps entre la résolution et l'ouverture de l'incident. Ces informations sont rangées dans une liste de la même longueur que le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c3f6f50335a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mresolved_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-2c3f6f50335a>\u001b[0m in \u001b[0;36mresolved_time\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#cols = ['opened_at','resolved_at']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mopened_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclosed_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#resolved_at = pd.to_datetime(row[cols[1]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, box, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mallow_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m         )\n\u001b[1;32m   1977\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m                                                       second=0, microsecond=0)\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Y_list = list()\n",
    "def resolved_time(row):    \n",
    "    cols = ['opened_at','closed_at']\n",
    "    #cols = ['opened_at','resolved_at']\n",
    "    opened_at = pd.to_datetime(row[cols[0]])\n",
    "    closed_at = pd.to_datetime(row[cols[1]])\n",
    "    #resolved_at = pd.to_datetime(row[cols[1]])\n",
    "    \n",
    "    time_completion = (closed_at - opened_at)\n",
    "    #time_completion = (resolved_at - opened_at)\n",
    "    \n",
    "    #if time_completion > datetime.timedelta(seconds=0): \n",
    "    #    Y_list.append(time_completion)\n",
    "    #else:\n",
    "    #    Y_list.append(-1)\n",
    "    Y_list.append(time_completion)\n",
    "    \n",
    "    \n",
    "for index, row in dataset.iterrows():\n",
    "    resolved_time(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. On controle s'il y a des valeurs négatives. Nous allons donc remplacer ces valeurs négatives par \"-1\" afin d'avoir des données logiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "minutesList = list()\n",
    "length = len(dataset) \n",
    "for i in range(length):\n",
    "    if Y_list[i] > datetime.timedelta(seconds=0):\n",
    "        minutesList.append(Y_list[i].total_seconds()/60)\n",
    "    else:\n",
    "        minutesList.append(-1)\n",
    "\n",
    "duration = np.ravel(pd.DataFrame(minutesList))\n",
    "duration[:20]\n",
    "dataset[\"resolved_in\"] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(columns=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. On garde seulement les incidents ayant pour statut \"Closed\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "resolved_temp = dataset.loc[dataset.incident_state == 2]\n",
    "dataset = pd.DataFrame(resolved_temp)\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On garde seulement les lignes ayant une valeur correcte pour resolved_in"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resolved_temp = dataset.loc[dataset.resolved_in != -1]\n",
    "dataset = pd.DataFrame(resolved_temp)\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir trouvé ces temps, on va convertir les colonnes de type date (en string) en float."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "dataset.opened_at = dataset.opened_at.str.replace(\"/\",\"\")\n",
    "dataset.sys_created_at = dataset.sys_created_at.str.replace(\"/\",\"\")\n",
    "dataset.sys_updated_at = dataset.sys_updated_at.str.replace(\"/\",\"\")\n",
    "dataset.resolved_at = dataset.resolved_at.str.replace(\"/\",\"\")\n",
    "dataset.closed_at = dataset.closed_at.str.replace(\"/\",\"\")\n",
    "\n",
    "dataset.opened_at = dataset.opened_at.str.replace(\" \",\"\")\n",
    "dataset.sys_created_at = dataset.sys_created_at.str.replace(\" \",\"\")\n",
    "dataset.sys_updated_at = dataset.sys_updated_at.str.replace(\" \",\"\")\n",
    "dataset.resolved_at = dataset.resolved_at.str.replace(\" \",\"\")\n",
    "dataset.closed_at = dataset.closed_at.str.replace(\" \",\"\")\n",
    "\n",
    "dataset.opened_at = dataset.opened_at.str.replace(\":\",\"\")\n",
    "dataset.sys_created_at = dataset.sys_created_at.str.replace(\":\",\"\")\n",
    "dataset.sys_updated_at = dataset.sys_updated_at.str.replace(\":\",\"\")\n",
    "dataset.resolved_at = dataset.resolved_at.str.replace(\":\",\"\")\n",
    "dataset.closed_at = dataset.closed_at.str.replace(\":\",\"\")\n",
    "\n",
    "dataset.opened_at = dataset.opened_at.astype(float)\n",
    "dataset.sys_created_at = dataset.sys_created_at.astype(float)\n",
    "dataset.sys_updated_at = dataset.sys_updated_at.astype(float)\n",
    "dataset.resolved_at = dataset.resolved_at.astype(float)\n",
    "dataset.closed_at = dataset.closed_at.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.opened_at = pd.to_datetime(dataset.opened_at, errors='coerce')\n",
    "dataset.sys_created_at = pd.to_datetime(dataset.sys_created_at, errors='coerce')\n",
    "dataset.sys_updated_at = pd.to_datetime(dataset.sys_updated_at, errors='coerce')\n",
    "dataset.resolved_at = pd.to_datetime(dataset.resolved_at, errors='coerce')\n",
    "dataset.closed_at = pd.to_datetime(dataset.closed_at, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. On regarde si certaines colonnes ont trop de valeurs manquantes (et ne sont donc pas utilisables). Si plus de 30% des données de la colonne sont manquantes, on la supprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "total = len(dataset[\"incident_state\"])\n",
    "#print(dataset[\"vendor\"][3])\n",
    "for column in dataset.columns:\n",
    "    v = 0\n",
    "    for i in range(total):\n",
    "        if dataset[column][i] == None:\n",
    "            v+=1\n",
    "        elif dataset[column].dtype == int or dataset[column].dtype == float:\n",
    "            if math.isnan(dataset[column][i]):\n",
    "                v+=1\n",
    "    if v/total*100 > 30:\n",
    "        print(\"Column name: \", column, \" ; Empty: \", v/total*100, \"%\")\n",
    "        dataset = dataset.drop(columns=column)\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour restaurer le dataset en cas de problème"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Régressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Si on a des valeurs infinies dans le dataset, elles sont remplacées par NaN. Ensuite, les valeurs NaN du dataset sont remplacées par une moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dataset[dataset==np.inf]=np.nan\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "\n",
    "#for i in range(len(dataset['sys_updated_at'])):\n",
    "#    dataset.sys_updated_at[i] = datetime.timestamp(dataset.sys_updated_at[i])\n",
    "#dataset.sys_updated_at = datetime.timestamp(dataset.sys_updated_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On crée nos X et Y en supprimant les colonnes nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.resolved_in\n",
    "x = dataset.drop('resolved_in', axis=1)\n",
    "x = x.drop('resolved_at', axis=1)\n",
    "x = x.drop('closed_at', axis=1)\n",
    "x = x.drop('opened_at', axis=1)\n",
    "x = x.drop('sys_created_at', axis = 1)\n",
    "x = x.drop('sys_updated_at', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "heat = dataset.drop('resolved_at', axis=1)\n",
    "heat = heat.drop('closed_at', axis=1)\n",
    "heat = heat.drop('opened_at', axis=1)\n",
    "heat = heat.drop('sys_created_at', axis = 1)\n",
    "heat = heat.drop('sys_updated_at', axis=1)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = heat.corr()\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. On sépare nos données en un dataset de train et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=0.2, random_state=0)\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. On réinitialise les index de test afin de pouvoir avoir des plots propres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reset_index(drop=True)\n",
    "Y_test = Y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test pour déterminer les variables donnant le meilleur score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "progress = 0\n",
    "newX = x\n",
    "bestScore = 0\n",
    "regress = DecisionTreeRegressor\n",
    "for var1 in range(len(x.columns)):\n",
    "    for var2 in range(len(x.columns)):\n",
    "        progress+=1\n",
    "        pr = progress/576*100\n",
    "        print(pr, '%')\n",
    "        \n",
    "        for var3 in range(len(x.columns)):\n",
    "            for var4 in range(len(x.columns)):\n",
    "                newX = x.iloc[: , [var1, var2, var3, var4]]\n",
    "                \n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(newX,y,test_size=0.2, random_state=0)\n",
    "                X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n",
    "                regressor = DecisionTreeRegressor()\n",
    "                regressor.fit(X_train, Y_train)\n",
    "                if regressor.score(X_test, Y_test) > bestScore:\n",
    "                    bestScore = regressor.score(X_test, Y_test)\n",
    "                    regress = regressor\n",
    "                    print(bestScore)\n",
    "                    print(newX[: 1])\n",
    "print(bestScore)\n",
    "newX\n",
    "#x = x[['knowledge', 'sys_mod_count', 'reassignment_count', 'resolved_by', 'assigned_to']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n",
    "\n",
    "dtm = DecisionTreeRegressor(max_depth=4,\n",
    "                           min_samples_split=5,\n",
    "                           max_leaf_nodes=10)\n",
    "\n",
    "dtm.fit(X_train,Y_train)\n",
    "print(\"R-Squared on train dataset={}\".format(dtm.score(X_test,Y_test)))\n",
    "\n",
    "dtm.fit(X_test,Y_test)   \n",
    "print(\"R-Squaredon test dataset={}\".format(dtm.score(X_test,Y_test)))\n",
    "\n",
    "param_grid = {\"criterion\": [\"mse\", \"mae\"],\n",
    "              \"min_samples_split\": [10, 20, 40],\n",
    "              \"max_depth\": [2, 6, 8],\n",
    "              \"min_samples_leaf\": [20, 40, 100],\n",
    "              \"max_leaf_nodes\": [5, 20, 100],\n",
    "              }\n",
    "\n",
    "grid_cv_dtm = GridSearchCV(dtm, param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "grid_cv_dtm.fit(X_train,Y_train)\n",
    "\n",
    "print(\"R-Squared::{}\".format(grid_cv_dtm.best_score_))\n",
    "print(\"Best Hyperparameters::\\n{}\".format(grid_cv_dtm.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs paramètres trouvés:\n",
    "{'criterion': 'mse', 'max_depth': 6, 'max_leaf_nodes': 100, 'min_samples_leaf': 20, 'min_samples_split': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#regressor = DecisionTreeRegressor(criterion='mse', max_depth=6, max_leaf_nodes=100, min_samples_leaf=20, min_samples_split=10)\n",
    "regressor = DecisionTreeRegressor()\n",
    "#cross_val_score(regressor, X_train, Y_train, cv=10)\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(regressor.feature_importances_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':y_pred})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2000:2100], Y_test[2000:2100], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2000:2100], y_pred[2000:2100], color=\"cornflowerblue\",\n",
    "         label=\"prediction\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rfr, param_distributions = random_grid, n_iter = 50, cv = 2, verbose=5, random_state=42, n_jobs = 4)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"R-Squared::{}\".format(rf_random.best_score_))\n",
    "print(\"Best Hyperparameters::\\n{}\".format(rf_random.best_params_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "R-Squared::0.21366126150335107\n",
    "Best Hyperparameters::\n",
    "{'n_estimators': 600, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "#randomForestRegressor = RandomForestRegressor(n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_features='sqrt', max_depth=110, bootstrap=False, random_state=100)\n",
    "randomForestRegressor = RandomForestRegressor()\n",
    "randomForestRegressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(randomForestRegressor.feature_importances_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yRFR = randomForestRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':yRFR})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, yRFR))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(Y_test, yRFR))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, yRFR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], yRFR[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"Random Forest Regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR Linear (Ne fonctionne pas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR\n",
    "#import numpy as np\n",
    "svrRegressor = SVR()\n",
    "svrRegressor.fit(X_train,Y_train)\n",
    "ySVR = svrRegressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "importances = pd.Series(svrRegressor.feature_importances_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svrRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':ySVR})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], ySVR[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction \", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"Support vector regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR RBF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVR\n",
    "rbfRegressor = SVR(kernel='rbf', epsilon=1.0)\n",
    "rbfRegressor.fit(X_train, Y_train)\n",
    "yRBF = rbfRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rbfRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':yRBF})\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], yRBF[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction \", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"Support vector regressor (RBF)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. AdaBoost Regressor with decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "adaBoostRegressor = AdaBoostRegressor(n_estimators=150,random_state=0)\n",
    "#adaBoostRegressor = AdaBoostRegressor()\n",
    "adaBoostRegressor.fit(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(adaBoostRegressor.feature_importances_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaBoostRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yAda = adaBoostRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':yAda})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], yAda[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"AdaBoost Regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. GradientBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gradientBoostRegressor = GradientBoostingRegressor(random_state=1, n_estimators=150)\n",
    "#gradientBoostRegressor = GradientBoostingRegressor()\n",
    "gradientBoostRegressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(gradientBoostRegressor.feature_importances_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientBoostRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yGrad = gradientBoostRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':yGrad})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], yGrad[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"GradientBoost Regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegressor = LinearRegression()\n",
    "linearRegressor.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(linearRegressor.singular_, index=X_train.columns)\n",
    "importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressor.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linGrad = linearRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Actual':Y_test, 'Predicted':linGrad})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2180], Y_test[2131:2180], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2180], linGrad[2131:2180], color=\"cornflowerblue\",\n",
    "         label=\"prediction\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"GradientBoost Regressor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Résultats finaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Comparaison des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test.index[2131:2145], Y_test[2131:2145], s=20, edgecolor=\"black\",\n",
    "            c=\"darkorange\", label=\"original data\")\n",
    "plt.plot(X_test.index[2131:2145], y_pred[2131:2145], color=\"red\",\n",
    "         label=\"prediction Decision Tree Regressor\", linewidth=1)\n",
    "plt.plot(X_test.index[2131:2145], yRFR[2131:2145], color=\"cornflowerblue\",\n",
    "         label=\"prediction Random Forest Regressor\", linewidth=1)\n",
    "#plt.plot(X_test.index[2131:2145], yRBF[2131:2145], color=\"green\",\n",
    "#         label=\"prediction RBF Regressor\", linewidth=1)\n",
    "plt.plot(X_test.index[2131:2145], yAda[2131:2145], color=\"purple\",\n",
    "         label=\"prediction AdaBoost Regressor\", linewidth=1)\n",
    "plt.plot(X_test.index[2131:2145], yGrad[2131:2145], color=\"yellow\",\n",
    "         label=\"prediction GradientBoost Regressor\", linewidth=1)\n",
    "plt.plot(X_test.index[2131:2145], linGrad[2131:2145], color=\"green\",\n",
    "         label=\"prediction Linear Regressor\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"Resolution time\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur ce graphe assez réduit, DecisionTreeRegressor et RandomForestRegressor ont les résultats les plus proches de la réalité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparons les scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Regressor: \", regressor.score(X_test, Y_test))\n",
    "print(\"Random Forest Regressor: \", randomForestRegressor.score(X_test, Y_test))\n",
    "#print(\"RBF Regressor: \", rbfRegressor.score(X_test, Y_test))\n",
    "print(\"AdaBoost Regressor: \", adaBoostRegressor.score(X_test, Y_test))\n",
    "print(\"GradientBoost Regressor: \", gradientBoostRegressor.score(X_test, Y_test))\n",
    "print(\"Linear Regressor: \", linearRegressor.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces résultats viennent confirmer le ressenti du graphe: Les scores les plus élevés sont Random Forest et Decision Tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
